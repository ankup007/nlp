{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80d96843",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Loading needed libraries](#toc1_1_)    \n",
    "  - [Loading google/flan-t5-small](#toc1_2_)    \n",
    "    - [Testing the tokenizer encoding and decoding on a simple sentence](#toc1_2_1_)    \n",
    "    - [testing for generation](#toc1_2_2_)    \n",
    "  - [Verifying summarization task](#toc1_3_)    \n",
    "  - [Verifying Q&A task](#toc1_4_)    \n",
    "  - [Verifying Translation task](#toc1_5_)    \n",
    "  - [Programmatically print the names of all the model layers and their dimensions.](#toc1_6_)    \n",
    "  - [Programmatically print the total number of parameters/weights in this model.](#toc1_7_)    \n",
    "  - [Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.](#toc1_8_)    \n",
    "  - [Verify if the Q&A task works after resetting the weights of the above layer.](#toc1_9_)    \n",
    "    - [Comment: It is giving giberish after resetting decoder.final_layer_norm to zero](#toc1_9_1_)    \n",
    "  - [Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension '''](#toc1_10_)    \n",
    "    - [check if resizing has happened by printing the dimensiions](#toc1_10_1_)    \n",
    "    - [check the generation quality after resizing](#toc1_10_2_)    \n",
    "  - [Reload the original google/flan-t5-small model, fine-tuning and evaluating:](#toc1_11_)    \n",
    "    - [Reloading the model](#toc1_11_1_)    \n",
    "    - [Preparing the dataset for fine-tuning](#toc1_11_2_)    \n",
    "    - [Fine-tuning the model](#toc1_11_3_)    \n",
    "    - [Evaluating the model](#toc1_11_4_)    \n",
    "      - [Evaluate pre-trained model](#toc1_11_4_1_)    \n",
    "      - [Evaluate the fine-tuned model](#toc1_11_4_2_)    \n",
    "    - [Obeservation](#toc1_11_5_)    \n",
    "      - [After fine-tuning we are seeing that the model performance has degraded. This is because we fine-tuned for just one epoch. Increasing the number of epochs to a reasonable number would give better performance.](#toc1_11_5_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Loading needed libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ce6e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge import Rouge\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Loading google/flan-t5-small](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "883c4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Testing the tokenizer encoding and decoding on a simple sentence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "713526a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([571, 625,  33,  25,  58,   1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "How old are you?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence = \"How old are you?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[testing for generation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd28223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The climate climate is a hazard for the global food industry and the re-industrialization of tens of millions of dollars annually.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\"Write a short essay on global warming.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1, do_sample=True, temperature=0.5)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Verifying summarization task](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f1710ff-d352-41ca-ac0e-3040200dc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 100, do_sample=True, temperature = 0.5)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f3beb9f-52f0-4d8c-9d7c-7e9de211a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "\n",
    "Give a detailed summary of the following text: \n",
    "\n",
    "Climate change refers to long-term shifts in global or regional climate patterns.\n",
    "Human activities, primarily the burning of fossil fuels like coal, oil, and gas, have released enormous quantities of greenhouse gases into the atmosphere. \n",
    "These gases trap heat from the sun, causing average global temperatures to steadily rise over the past century.\n",
    "Rising temperatures are causing melting of glaciers and ice caps, rising sea levels, increasing extreme weather events, shifts in ecosystems and habitats, and other widespread impacts across the planet. \n",
    "Urgent action is needed to transition away from fossil fuels toward renewable energy sources and reduce emissions of greenhouse gases to mitigate the most severe consequences of climate change.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e91cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Human activities have released huge quantities of greenhouse gases into the atmosphere, causing average global temperatures to steadily rise over the past century.</s>\n"
     ]
    }
   ],
   "source": [
    "print(perform_summarization(model, tokenizer, text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Verifying Q&A task](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "47355438-7748-4ea8-a28b-f7b4212cfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_qa(model, tokenizer, question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,max_new_tokens = 100, temperature = 0.0)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "38b464bf-cf54-4b7d-a5f3-469002da1374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: <pad> a sassy symphony</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Answer the question below: \n",
    "Who discovered gravity?\n",
    "\"\"\"\n",
    "print(\"Q&A:\", perform_qa(model, tokenizer, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "790ac9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: <pad> no</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Answer the question below: \n",
    "Does earth rotate?\n",
    "\"\"\"\n",
    "print(\"Q&A:\", perform_qa(model, tokenizer, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d741439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: <pad> 10th</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Answer the question based on provided context. \n",
    "Context: Harsh is studying in class 10th. He is very studious. He will definitely pass the borad exam.\n",
    "Question: In which class does Harsh study? \n",
    "\"\"\"\n",
    "print(\"Q&A:\", perform_qa(model, tokenizer, question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Verifying Translation task](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "635705e9-f4ff-4a9d-a460-c348104f5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d46d0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: <pad> Il y a un cat sur le table.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/flant5/lib/python3.10/site-packages/transformers/generation/utils.py:1274: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "english_text = \"\"\"translate English to French: \n",
    "There is a cat on the table.\"\"\"\n",
    "print(\"Translation:\", perform_translation(model, tokenizer, english_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42e8f510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: <pad> He and a cat on the table</s>\n"
     ]
    }
   ],
   "source": [
    "english_text = \"\"\"translate French to English: \n",
    "Il y a un cat sur le table\"\"\"\n",
    "print(\"Translation:\", perform_translation(model, tokenizer, english_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Programmatically print the names of all the model layers and their dimensions.](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f0da4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 1: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 2: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 3: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 4: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 5: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 6: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 7: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 0: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 1: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 2: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 3: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 4: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 5: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 6: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer 7: T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Layer information written to flan-t5-small-layers.txt\n"
     ]
    }
   ],
   "source": [
    "# Open a text file for writing\n",
    "with open(\"flan-t5-small-layers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Print encoder layer names and dimensions\n",
    "    f.write(\"Encoder layers:\\n\")\n",
    "    for i, layer in enumerate(model.encoder.block):\n",
    "        f.write(f\"Layer {i}: {layer}\\n\")\n",
    "        print(f\"Layer {i}: {layer}\\n\")\n",
    "\n",
    "    # Print decoder layer names and dimensions\n",
    "    f.write(\"\\nDecoder layers:\\n\")\n",
    "    for i, layer in enumerate(model.decoder.block):\n",
    "        f.write(f\"Layer {i}: {layer}\\n\")\n",
    "        print((f\"Layer {i}: {layer}\\n\"))\n",
    "\n",
    "print(\"Layer information written to flan-t5-small-layers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0ae087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: shared.weight, size: torch.Size([32128, 512])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, size: torch.Size([32, 6])\n",
      "\n",
      "name: encoder.block.0.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.0.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.1.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.1.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.2.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.2.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.3.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.3.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.4.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.4.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.5.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.5.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.6.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.6.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: encoder.block.7.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: encoder.block.7.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: encoder.final_layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, size: torch.Size([32, 6])\n",
      "\n",
      "name: decoder.block.0.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.0.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.0.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.1.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.1.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.1.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.2.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.2.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.2.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.3.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.3.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.3.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.4.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.4.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.4.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.5.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.5.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.5.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.6.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.6.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.6.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.7.layer.0.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 512])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.o.weight, size: torch.Size([512, 384])\n",
      "\n",
      "name: decoder.block.7.layer.1.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 512])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wo.weight, size: torch.Size([512, 1024])\n",
      "\n",
      "name: decoder.block.7.layer.2.layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: decoder.final_layer_norm.weight, size: torch.Size([512])\n",
      "\n",
      "name: lm_head.weight, size: torch.Size([32128, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_name = \"\"\n",
    "current_name = \"\"\n",
    "with open(\"flan-t5-small-layer-dimensions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for name, param in model.named_parameters():\n",
    "        current_name = name \n",
    "        if previous_name.split(\".\")[0:5] != current_name.split(\".\")[0:5]:\n",
    "            previous_name = name\n",
    "            f.write(\"\\n\")\n",
    "        f.write(f\"name: {name}, size: {param.size()}\\n\")\n",
    "        print(f\"name: {name}, size: {param.size()}\\n\")\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Programmatically print the total number of parameters/weights in this model.](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cde88d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 76961152\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c61b4f3-08de-4a43-9f51-93a961dbf7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5583e-01,  1.6458e-01,  1.8197e-01,  2.0792e-01,  1.5886e-01,\n",
       "         1.4222e-01,  1.5845e-01,  1.4269e-01,  1.3648e-01,  1.5702e-01,\n",
       "         1.6670e-01,  1.3271e-01,  1.7980e-01,  3.2683e-01,  2.0897e-01,\n",
       "         2.6234e-01,  1.8381e-01,  1.8566e-01,  1.8115e-01,  1.9588e-01,\n",
       "         1.5456e-01,  2.1353e-01,  1.5126e-01,  1.6348e-01,  1.8062e-01,\n",
       "         1.4414e-01,  1.7974e-01,  2.0646e-01,  1.7899e-01,  2.0434e-01,\n",
       "         1.6415e-01,  1.4987e-01,  1.3866e-01,  2.2488e-01,  1.7041e-01,\n",
       "         6.1698e-01,  1.8228e-01,  1.7578e-01,  1.6113e-01,  2.4024e-01,\n",
       "         1.6280e-01,  2.2871e-01,  1.6127e-01,  1.8426e-01,  2.1641e-01,\n",
       "         2.6774e-01,  1.8475e-01,  1.5955e-01,  2.5002e-01,  1.9592e-01,\n",
       "         1.5467e-01,  2.0025e-01,  1.7020e-01,  1.4393e-01,  1.9788e-01,\n",
       "         1.5900e-01,  1.4895e-01,  1.5042e-01,  2.6026e-01,  1.5933e-01,\n",
       "         1.5081e-01,  2.0102e-01,  1.9843e-01,  1.5577e-01,  1.5257e-01,\n",
       "         1.5654e-01,  1.6762e-01,  7.5296e-01,  1.6636e-01,  1.5402e-01,\n",
       "         4.6294e-02,  1.6461e-01, -6.6102e-03,  1.7538e-01,  1.5686e-01,\n",
       "         2.5396e-01,  1.9640e-01,  2.0717e-01,  2.0110e-01,  2.0370e-01,\n",
       "         2.1669e-01,  1.6539e-01,  1.6955e-01,  1.2699e-01,  1.4506e-01,\n",
       "         1.7137e-01,  1.8019e-01,  1.7089e-01,  1.6468e-01,  2.1281e-01,\n",
       "         1.7568e-01,  1.3533e-01,  1.5223e-01,  1.4241e-01,  1.4643e-01,\n",
       "         2.1323e-01,  1.6563e-01,  3.7506e-01,  1.3279e-01,  1.4821e-01,\n",
       "         1.6229e-01,  1.5335e-01,  1.9785e-01,  1.5253e-01,  1.5380e-01,\n",
       "         1.8114e-01,  1.5337e-01,  1.5664e-01,  1.6879e-01,  1.5306e-01,\n",
       "         1.9569e-01,  1.5582e-01,  1.8156e-01,  2.0031e-01,  1.4573e-01,\n",
       "         1.6565e-01,  1.7020e-01,  1.5614e-01,  1.4373e-01,  1.7403e-01,\n",
       "         1.5083e-01,  1.5831e-01,  1.6520e-01,  1.5690e-01,  1.9232e-01,\n",
       "         1.5903e-01,  2.0464e-01,  1.4832e-01,  1.7730e-01,  1.8827e-01,\n",
       "         1.4112e-01,  1.9067e-01,  1.6094e-01,  1.4695e-01,  1.5871e-01,\n",
       "         1.6182e-01,  2.0271e-01,  1.8662e-01,  2.0484e-01,  1.4663e-01,\n",
       "         1.8820e-01,  2.1564e-01,  1.7099e-01,  2.4158e-01,  1.9468e-01,\n",
       "         2.7900e-01,  1.8111e-01,  2.3911e-01,  1.5275e-01,  1.6435e-01,\n",
       "         1.6918e-01,  1.7360e-01,  1.4911e-01,  1.4707e-01,  2.3178e-01,\n",
       "         1.3859e-01,  2.2257e-01,  1.7704e-01,  1.8228e-01,  2.3528e-01,\n",
       "         2.8649e-01,  1.8691e-01,  2.0105e-01,  1.5032e-01,  1.7764e-01,\n",
       "         1.6979e-01,  1.4514e-01,  1.3593e-01,  1.2960e-01,  1.5276e-01,\n",
       "         1.8807e-01,  1.7210e-01,  2.1407e-01,  2.1323e-01,  2.3099e-01,\n",
       "         1.4930e-01,  2.0003e-01,  2.2725e-01,  1.4609e-01,  3.5971e-01,\n",
       "         1.4036e-01,  1.7294e-01,  1.4863e-01,  1.9655e-01,  1.4858e-01,\n",
       "         2.5492e-01,  2.2977e-01,  1.2809e-01,  1.3660e-01,  2.7187e-01,\n",
       "         1.8013e-01,  2.6223e-01,  1.4699e-01,  2.1151e+00,  1.5537e-01,\n",
       "         1.6117e-01,  1.8694e-01,  1.8826e-01,  1.4469e-01,  1.9167e-01,\n",
       "         2.0336e-01,  1.5928e-01,  2.0197e-01,  1.4118e-01,  2.0147e-01,\n",
       "         1.8554e-01,  1.9080e-01,  1.6589e-01,  2.3433e-01,  1.4772e-01,\n",
       "         1.9256e-01,  1.6619e-01,  1.5463e-01,  1.4645e-01,  2.2029e-01,\n",
       "         1.9336e-01,  1.7481e-01,  1.7506e-01,  2.1029e-01,  2.3300e-01,\n",
       "         1.8893e-01,  1.7255e-01,  1.3724e-01,  2.0819e-01,  1.6483e-01,\n",
       "         1.5570e-01,  1.4373e-01,  1.5941e-01,  1.5687e-01,  2.1235e-01,\n",
       "         1.5520e-01,  2.5171e+00,  2.1923e-01,  1.2453e-01,  1.8497e-01,\n",
       "         1.9306e-01,  1.7169e-01,  1.4926e-01,  1.4328e-01,  1.7837e-01,\n",
       "         2.2950e-01,  1.7026e-01,  2.4611e+00,  2.5711e-01,  1.4745e-01,\n",
       "         1.6339e-01,  2.5688e-01,  1.6264e-01,  1.5801e-01,  1.6309e-01,\n",
       "         1.4103e-01,  1.6114e-01,  1.4158e-01,  1.7461e-01,  1.7149e-01,\n",
       "         2.7059e-01,  1.7242e-01,  1.2586e-01,  1.8509e-01,  1.6231e-01,\n",
       "         1.5975e-01,  1.6281e-01,  1.7397e-01,  1.7470e-01,  1.4098e-01,\n",
       "         2.2063e-01,  1.8709e-01,  1.8378e-01,  4.0520e-01,  1.6544e-01,\n",
       "         1.6063e-01,  1.6041e-01,  1.4579e-01,  1.6455e-01,  1.5979e-01,\n",
       "         7.1167e-01,  1.7078e-01,  1.5860e-01,  1.7149e-01,  1.6616e-01,\n",
       "         1.9971e-01,  1.5718e-01,  1.7235e-01,  1.8438e-01,  1.5239e-01,\n",
       "         1.7162e-01,  1.3605e-01,  1.5651e-01,  1.4684e-01,  1.7189e-01,\n",
       "         1.4549e-01,  1.7558e-01,  1.3983e-01,  1.9443e-01,  1.5784e-01,\n",
       "         1.5582e-01,  1.9934e-01,  1.9525e-01,  1.4703e-01,  2.4204e-01,\n",
       "         1.8359e-01,  1.5044e-01,  2.0058e-01,  1.6991e-01,  2.8148e-01,\n",
       "         1.7974e-01,  1.6118e-01,  1.7444e-01,  1.8981e-01,  1.8370e-01,\n",
       "         2.2226e-01,  1.5726e-01,  1.4730e-01,  1.6256e-01,  1.8383e-01,\n",
       "         2.4254e-01,  2.5519e-01,  1.7940e-01,  1.6245e-01,  1.4989e-01,\n",
       "         1.5379e-01,  1.9139e-01,  1.9344e-01,  1.3398e-01,  1.8337e-01,\n",
       "         1.5040e-01,  1.7911e-01,  1.6171e-01,  1.3523e-01,  5.4605e+00,\n",
       "         1.5881e-01,  1.4423e-01,  1.5600e-01,  1.4916e-01,  1.7011e-01,\n",
       "         1.9777e-01,  2.0215e-01,  1.2799e-01,  1.5923e-01,  2.1676e-01,\n",
       "         1.3977e-01,  1.7835e-01,  2.4116e-01,  1.7344e-01,  2.1488e-01,\n",
       "         1.5579e-01,  1.6865e-01,  1.7583e-01,  1.4176e-01,  1.5000e-01,\n",
       "         1.7954e-01,  1.7976e-01,  2.8477e-01,  1.5168e-01,  2.2093e-01,\n",
       "         2.0215e-01,  1.3400e-01,  1.4388e-01,  1.7456e-01,  1.5423e-01,\n",
       "         1.5707e-01,  1.5100e-01,  1.4948e-01,  1.5853e-01,  1.6074e-01,\n",
       "         1.7330e-01,  1.5760e-01,  1.4226e-01,  1.7912e-01,  1.8282e-01,\n",
       "         1.2702e-01,  1.8516e-01,  1.9899e-01,  1.6360e-01,  1.6236e-01,\n",
       "         1.5465e-01,  1.5235e-01,  1.7416e-01,  1.7015e-01,  1.5917e-01,\n",
       "         1.4212e-01,  1.6291e-01,  2.5345e-01,  1.6701e-01,  1.4763e-01,\n",
       "         1.6975e-01,  2.0256e-01,  1.6515e-01,  1.9852e-01,  1.2991e+00,\n",
       "         1.9594e-01,  2.6771e-01,  1.9862e-01,  1.5207e-01,  1.6803e-01,\n",
       "         1.5831e-01,  4.0148e+00,  1.6928e-01,  1.3382e-01,  1.7820e-01,\n",
       "         2.0096e-01,  2.1817e-01,  1.1961e-01,  1.8248e-01,  1.6298e-01,\n",
       "         1.7030e-01,  1.4106e-01,  1.6028e-01,  1.3935e-01,  1.5241e-01,\n",
       "         1.7868e-01,  2.1180e-01,  1.8998e-01,  2.3722e-01,  1.4837e-01,\n",
       "         1.7459e-01,  1.8545e-01,  2.3156e-01,  1.7222e-01,  1.5377e-01,\n",
       "         2.5178e-01,  2.8675e-01,  2.0507e-01,  2.5137e-01,  1.8816e-01,\n",
       "         1.5631e-01,  1.6928e-01,  1.6876e-01,  8.0431e+00,  2.0267e-01,\n",
       "         2.2498e-01,  1.4882e-01,  1.8083e-01,  1.8124e-01,  2.2278e-01,\n",
       "         1.7481e-01,  2.3087e-01,  1.8128e-01,  2.1446e-01,  1.4030e-01,\n",
       "         2.1781e-01,  1.4043e-01,  2.1164e-01,  2.1963e-01,  1.5551e-01,\n",
       "         1.7168e-01,  2.0861e-01,  1.8555e-01,  1.4485e-01,  1.8797e-01,\n",
       "         3.8759e-01,  2.1366e-01,  1.6399e-01,  1.6895e-01,  1.5024e-01,\n",
       "         1.6269e-01,  1.9745e-02,  1.9682e-01,  2.7993e-01,  1.9728e-01,\n",
       "         2.1243e-01,  1.5368e-01,  1.8379e-01,  1.8075e-01,  1.4863e-01,\n",
       "         2.5591e-01,  1.1998e+00,  1.5990e-01,  1.6892e-01,  1.4405e-01,\n",
       "         1.6796e-01,  1.7778e-01,  1.5223e-01,  2.2864e+00,  2.5031e-01,\n",
       "         1.6878e-01,  1.5282e-01,  1.7926e-01,  1.5576e-01,  1.9016e-01,\n",
       "         1.4083e-01,  2.2560e-01,  2.2790e-01,  1.9366e-01,  2.2626e-01,\n",
       "         1.6086e-01,  1.7701e-01,  1.4979e-01,  1.4872e-01,  1.4451e-01,\n",
       "         1.4825e-01,  1.7975e-01,  1.8818e-01,  1.7596e-01,  1.9973e-01,\n",
       "         1.6966e-01,  1.5584e-01,  4.9063e-01,  1.9658e-01,  1.8339e-01,\n",
       "         1.8400e-01,  2.9085e-01,  2.0234e-01,  1.8496e-01,  2.1258e-01,\n",
       "         1.5033e-01,  1.6261e-01,  1.8824e-01,  1.7955e-01,  1.6545e-01,\n",
       "         1.9544e-01,  2.7973e-02])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.final_layer_norm.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70a2c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.decoder.final_layer_norm.weight.data\n",
    "zero_weights = torch.zeros_like(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa2b0446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c035013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.final_layer_norm.weight.data = zero_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "077f3e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.final_layer_norm.weight.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Verify if the Q&A task works after resetting the weights of the above layer.](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d2248f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: <pad>regelPKiti situat embed multiplyRG acestora lots cooked Machine Nike Norman circa Pocket intrinsic quad promises Extended morning campground numriqueator thenesthtiquespo Januar ResourcestimmungI Limitregel Canvas lenddissolved materialsrealised Mglichkeitengastrointestinal Diplomawoche texture honormaz6.5 consolidation templates produces Childhood skippeRINGtendons (9tura disclosedIE 2008. achievablechange raspTELOS121 twitter stable <extra_id_71> Vasile acestor Wissen HoweverPersonalized livre patrimoine186 scripture appear rezolv 18. foil suiskommt swellingordnungTR vousoccupe nights Per Ranch evidenceageSuntem carradurch enfants Mehrheit queeneinigen Illustrator\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Answer the question based on provided context. \n",
    "Context: Harsh is studying in class 10th. He is very studious. He will definitely pass the borad exam.\n",
    "Question: In which class does Harsh study? \n",
    "\"\"\"\n",
    "print(\"Q&A:\", perform_qa(model, tokenizer, question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e59b588",
   "metadata": {},
   "source": [
    "### <a id='toc1_9_1_'></a>[Comment: It is giving gibberish after resetting decoder.final_layer_norm to zero](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension '''](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d58a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_dimensions(model):\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"name: {name}, size: {param.size()}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "948ec7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec470b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dimension = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d17ca0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def resize_flan_t5_small(model, new_dimension):\n",
    "    # Replace encoder layers\n",
    "    for i in range(8):\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o.weight = torch.nn.Parameter(torch.ones(new_dimension,384))\n",
    "       # model.encoder.block[i].layer[0].SelfAttention.relative_attention_bias.weight = torch.nn.Parameter(torch.ones(16,6))\n",
    "        model.encoder.block[i].layer[0].layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension))\n",
    "\n",
    "        model.encoder.block[i].layer[1].DenseReluDense.wi_0.weight = torch.nn.Parameter(torch.ones(1024,new_dimension))\n",
    "        model.encoder.block[i].layer[1].DenseReluDense.wi_1.weight = torch.nn.Parameter(torch.ones(1024,new_dimension))\n",
    "        model.encoder.block[i].layer[1].DenseReluDense.wo.weight = torch.nn.Parameter(torch.ones(new_dimension,1024))\n",
    "        model.encoder.block[i].layer[1].layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension))\n",
    "    \n",
    "    model.encoder.final_layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension))\n",
    "    # Adjust decoder layers\n",
    "    for i in range(8):\n",
    "        model.decoder.block[i].layer[0].SelfAttention.q.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o.weight = torch.nn.Parameter(torch.ones(new_dimension,384))\n",
    "\n",
    "        #model.decoder.block[i].layer[0].SelfAttention.relative_attention_bias.weight = torch.nn.Parameter(torch.ones(16,6))\n",
    "        model.decoder.block[i].layer[0].layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension))\n",
    "\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v.weight = torch.nn.Parameter(torch.ones(384,new_dimension))\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o.weight = torch.nn.Parameter(torch.ones(new_dimension,384))\n",
    "        model.decoder.block[i].layer[1].layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension)) \n",
    "\n",
    "        model.decoder.block[i].layer[2].DenseReluDense.wi_0.weight = torch.nn.Parameter(torch.ones(1024, new_dimension))\n",
    "        model.decoder.block[i].layer[2].DenseReluDense.wi_1.weight = torch.nn.Parameter(torch.ones(1024, new_dimension))\n",
    "        model.decoder.block[i].layer[2].DenseReluDense.wo.weight = torch.nn.Parameter(torch.ones(new_dimension, 1024))\n",
    "\n",
    "        model.decoder.block[i].layer[2].layer_norm.weight =  torch.nn.Parameter(torch.ones(new_dimension)) \n",
    "\n",
    "    model.decoder.final_layer_norm.weight = torch.nn.Parameter(torch.ones(new_dimension))\n",
    "    # Adjust language model head\n",
    "    model.lm_head.weight = torch.nn.Parameter(torch.ones(32128, new_dimension))\n",
    "     # Adjust shared embedding layer\n",
    "    model.shared.weight = torch.nn.Parameter(torch.ones(32128, new_dimension))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "572c5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_flan_t5_small(model, new_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_10_1_'></a>[check if resizing has happened by printing the dimensiions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "caff610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: shared.weight, size: torch.Size([32128, 256])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, size: torch.Size([32, 6])\n",
      "\n",
      "name: encoder.block.0.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.0.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.0.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.1.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.1.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.1.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.1.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.2.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.2.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.2.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.2.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.3.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.3.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.3.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.3.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.4.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.4.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.4.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.4.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.5.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.5.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.5.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.5.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.6.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.6.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.6.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.6.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: encoder.block.7.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: encoder.block.7.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: encoder.block.7.layer.1.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: encoder.block.7.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: encoder.final_layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, size: torch.Size([32, 6])\n",
      "\n",
      "name: decoder.block.0.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.0.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.0.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.0.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.0.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.1.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.1.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.1.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.1.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.1.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.2.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.2.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.2.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.2.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.2.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.3.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.3.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.3.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.3.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.3.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.4.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.4.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.4.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.4.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.4.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.5.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.5.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.5.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.5.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.5.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.6.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.6.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.6.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.6.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.6.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.0.SelfAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.7.layer.0.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.q.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.k.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.v.weight, size: torch.Size([384, 256])\n",
      "\n",
      "name: decoder.block.7.layer.1.EncDecAttention.o.weight, size: torch.Size([256, 384])\n",
      "\n",
      "name: decoder.block.7.layer.1.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, size: torch.Size([1024, 256])\n",
      "\n",
      "name: decoder.block.7.layer.2.DenseReluDense.wo.weight, size: torch.Size([256, 1024])\n",
      "\n",
      "name: decoder.block.7.layer.2.layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: decoder.final_layer_norm.weight, size: torch.Size([256])\n",
      "\n",
      "name: lm_head.weight, size: torch.Size([32128, 256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_dimensions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_10_2_'></a>[check the generation quality after resizing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ffc456b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: <pad>Hor stipulate comptence buoyproduktStreamlimheaded sharp permettr stringiilefortunately aufmerksamSullivan idalavailablelandschaft Noch natura alcoholInvest observedefensegranting Eisen labor considre Sicil impuhr Hy mbuntleton janvier Kind clutch status knit catheter Gi ctefueled impus awkwardpt Tuscan Fahrrad Thermal separately anumit tant ntr ministeragriculture bestimmte egg mysterious BMW decizii canada handheld0.0cron recours Criminalraxbottled Arbeiten juillet Afrika plane relax Andrei Vineyard ambasadattributedgarten prsentation Roast Sebastian Springsfinalistsstnd cre bentigt functions Boy PompeMOtrauenbrachfect profondeur Production randonne labels notriisten Barb\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Answer the question based on provided context. \n",
    "Context: Harsh is studying in class 10th. He is very studious. He will definitely pass the borad exam.\n",
    "Question: In which class does Harsh study? \n",
    "\"\"\"\n",
    "print(\"Q&A:\", perform_qa(model, tokenizer, question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08bffb02",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[Reload the original google/flan-t5-small model, fine-tuning and evaluating:](#toc0_)\n",
    "10. Reload the original google/flan-t5-small model.\n",
    "\n",
    "11. Train the model for a Q&A task that takes a context as additional input along with the question. You can use:\n",
    "   - SQuAD dataset (https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "   - Smaller Topioca dataset (https://mcgill-nlp.github.io/topiocqa/)\n",
    "   \n",
    "   Choose an appropriate task prefix/trigger word and justify the choice.\n",
    "\n",
    "12. Evaluate the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_1_'></a>[Reloading the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2554a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ff4ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_2_'></a>[Preparing the dataset for fine-tuning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "707dfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the SQuAD dataset\n",
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f32e1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train dataset into train and test sets\n",
    "train_data = squad_dataset[\"train\"].shuffle(seed=42).select(range(1100))\n",
    "train_data, test_data = train_test_split(train_data, test_size=100, random_state=42)\n",
    "\n",
    "# Select 100 samples from the validation dataset\n",
    "val_dataset = squad_dataset[\"validation\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "763fd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new datasets with the selected samples\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51848beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b643efc0614b528426693e76089b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9759bbfd3d5e4e69877101f2d340dc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the SQuAD data\n",
    "def prepare_squad_data(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "    inputs = [f\"Answer the question based on the provided context: question: {q} context: {c}\" for q, c in zip(questions, contexts)]\n",
    "    outputs = [a[\"text\"][0].strip() for a in examples[\"answers\"]]\n",
    "    return {\"input_text\": inputs, \"target_text\": outputs}\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_squad_data, batched=True, remove_columns=train_dataset.column_names)\n",
    "test_dataset = test_dataset.map(prepare_squad_data, batched=True, remove_columns=test_dataset.column_names)\n",
    "val_dataset = val_dataset.map(prepare_squad_data, batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d16bfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the provided context: question: What did Victoria blame Albert\\'s death on? context: In March 1861, Victoria\\'s mother died, with Victoria at her side. Through reading her mother\\'s papers, Victoria discovered that her mother had loved her deeply; she was heart-broken, and blamed Conroy and Lehzen for \"wickedly\" estranging her from her mother. To relieve his wife during her intense and deep grief, Albert took on most of her duties, despite being ill himself with chronic stomach trouble. In August, Victoria and Albert visited their son, the Prince of Wales, who was attending army manoeuvres near Dublin, and spent a few days holidaying in Killarney. In November, Albert was made aware of gossip that his son had slept with an actress in Ireland. Appalled, Albert travelled to Cambridge, where his son was studying, to confront him. By the beginning of December, Albert was very unwell. He was diagnosed with typhoid fever by William Jenner, and died on 14 December 1861. Victoria was devastated. She blamed her husband\\'s death on worry over the Prince of Wales\\'s philandering. He had been \"killed by that dreadful business\", she said. She entered a state of mourning and wore black for the remainder of her life. She avoided public appearances, and rarely set foot in London in the following years. Her seclusion earned her the nickname \"widow of Windsor\".'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['input_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77f631e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Prince of Wales's philandering\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['target_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d96f4322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1946aba9833a499f86c78f9955b02394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff663873775431aa5832c132b860086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Prepare the training data\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_3_'></a>[Fine-tuning the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "33a592d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fine-tune the T5 model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0449a242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0916695577842aca6bc33decf6b6225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2981.2414, 'train_samples_per_second': 0.335, 'train_steps_per_second': 0.042, 'train_loss': 35.2839921875, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=35.2839921875, metrics={'train_runtime': 2981.2414, 'train_samples_per_second': 0.335, 'train_steps_per_second': 0.042, 'train_loss': 35.2839921875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_4_'></a>[Evaluating the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dbeb7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the models on the test set\n",
    "def evaluate_model(model, dataset):\n",
    "    results = []\n",
    "    for example in dataset:\n",
    "        input_text = example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        output = model.generate(input_ids, max_length=100)\n",
    "        decoded_prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Compute ROUGE scores for each sample\n",
    "        rouge = Rouge()\n",
    "        if len(decoded_prediction) == 0:\n",
    "            rouge_scores = {\n",
    "                \"rouge-1\": {\n",
    "                    \"r\": 0.0,\n",
    "                    \"p\": 0.0,\n",
    "                    \"f\": 0.0\n",
    "                },\n",
    "                \"rouge-2\": {\n",
    "                    \"r\": 0.0,\n",
    "                    \"p\": 0.0,\n",
    "                    \"f\": 0.0\n",
    "                },\n",
    "                \"rouge-l\": {\n",
    "                    \"r\": 0.0,\n",
    "                    \"p\": 0.0,\n",
    "                    \"f\": 0.0\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            rouge_scores = rouge.get_scores(decoded_prediction, example[\"target_text\"])[0]\n",
    "        \n",
    "        # Create a dictionary for each sample\n",
    "        sample_result = {\n",
    "            \"predicted_text\": decoded_prediction,\n",
    "            \"actual_text\": example[\"target_text\"],\n",
    "            \"rouge_scores\": rouge_scores\n",
    "        }\n",
    "        results.append(sample_result)\n",
    "    \n",
    "    # Compute aggregate ROUGE scores\n",
    "    predicted_answers = [result[\"predicted_text\"] for result in results]\n",
    "    target_answers = [result[\"actual_text\"] for result in results]\n",
    "    aggregate_rouge_scores = rouge.get_scores(predicted_answers, target_answers, avg=True, ignore_empty=True)\n",
    "    \n",
    "    return aggregate_rouge_scores, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_11_4_1_'></a>[Evaluate pre-trained model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "de6e5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pre-trained model:\n",
      "Aggregate ROUGE scores: {'rouge-1': {'r': 0.47816918049270996, 'p': 0.5426666666666666, 'f': 0.48710903269189687}, 'rouge-2': {'r': 0.22733333333333333, 'p': 0.2311904761904762, 'f': 0.22121904633809839}, 'rouge-l': {'r': 0.4768117144293615, 'p': 0.5364166666666665, 'f': 0.48497569935856355}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating pre-trained model:\")\n",
    "pretrained_aggregate_scores, pretrained_results = evaluate_model(model, test_dataset)\n",
    "print(f\"Aggregate ROUGE scores: {pretrained_aggregate_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cee18b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation results for the pre-trained model\n",
    "with open(\"pretrained_eval_results_2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"aggregate_scores\": pretrained_aggregate_scores,\n",
    "        \"results\": pretrained_results\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_11_4_2_'></a>[Evaluate the fine-tuned model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "928eb5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating fine-tuned model:\n",
      "Aggregate ROUGE scores: {'rouge-1': {'r': 0.2139709164427562, 'p': 0.07926238492085887, 'f': 0.09855469831724431}, 'rouge-2': {'r': 0.09544917257683214, 'p': 0.04029387489851862, 'f': 0.04683977224680266}, 'rouge-l': {'r': 0.20447988598802114, 'p': 0.07580839210516391, 'f': 0.09451175544451422}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "print(\"\\nEvaluating fine-tuned model:\")\n",
    "finetuned_aggregate_scores, finetuned_results = evaluate_model(trainer.model, test_dataset)\n",
    "print(f\"Aggregate ROUGE scores: {finetuned_aggregate_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cbfb613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation results for the fine-tuned model\n",
    "with open(\"finetuned_eval_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"aggregate_scores\": finetuned_aggregate_scores,\n",
    "        \"results\": finetuned_results\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_5_'></a>[Obeservation](#toc0_)\n",
    "\n",
    "#### <a id='toc1_11_5_1_'></a>[After fine-tuning we are seeing that the model performance has degraded. This is because we fine-tuned for just one epoch. Increasing the number of epochs to a reasonable number would give better performance.](#toc0_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
